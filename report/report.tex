\documentclass[journal, 12pt]{IEEEtran}
%\usepackage[a4paper, total={6in, 8in}]{geometry}
\raggedbottom

% *** CITATION PACKAGES ***
%
%\usepackage{cite}
\usepackage{capt-of}%%To get the caption
\usepackage{gensymb}
\usepackage{graphicx} %package to manage images
\graphicspath{ {./images/} }
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{graphicx, caption, subcaption}
\usepackage{subfig}
\usepackage{graphicx,subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{lipsum}

\usepackage[style=ieee]{biblatex}
\DeclareLanguageMapping{english}{english-apa}
\addbibresource{references.bib}
\usepackage[justification=centering]{caption}

\usepackage{setspace}

\usepackage{hhline}


\usepackage{changepage} 

\usepackage{booktabs}
\usepackage{xcolor}

\usepackage{makecell}
\usepackage{graphicx,subcaption}
\usepackage{listings}
\renewcommand\theadfont{}
\DeclareMathOperator{\EX}{\mathbb{E}}% expected value
\newcommand{\cc}[1]{\texttt{#1}}


\onecolumn
\renewcommand{\baselinestretch}{1.1} 

\begin{document}


\pagenumbering{gobble}
%\clearpage\mbox{} % adds and empty page
%\clearpage
\pagenumbering{arabic}
\setcounter{page}{1}

\title{{\large Mini Project 3: Predicting Ridership and Hottest Pickup Zones in January \\ Based on Weather Data}}

\author{ENGR-UH 4560 Machine Learning, Fall 2019\\
\medskip
Barkin Simsek,~\IEEEmembership{bs3528@nyu.edu};
Nishant Aswani,~\IEEEmembership{nsa325@nyu.edu}}% <-this % stops a space


% The paper headers
\markboth{Simsek, Aswani, ENGR-UH 4560 Machine Learning, Fall 2019}%
{}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
With the rapid development of modern cities, everyday more and more people use taxicabs. Coordinating taxicabs in the city and meeting the demand of the passengers is real world a problem that needs to be addressed in the New York City. Due to its complex structure, it is a problem that is suitable for using Machine Learning. In this project we aimed to create a model for the yellow taxicabs in New York City that the taxi service providers can predict the number of pickups in a location in New York City based on the weather conditions, pick-up location and the day of the week. After the experimentation process, it is found that XGBoost algorithm performed the best with a mean absolute percentage error of 79.92\% and Random Forest performed worst with a mean absolute percentage error of 82.58\%.


\end{abstract}

%%%%%%%%%%%%%%%%%%
%% Introduction %%
%%%%%%%%%%%%%%%%%%
\section{Introduction}
\subsection{Research Motivation}
\noindent A taxicab (or taxi as commonly known) is a private type of vehicle, where passengers decide on the pick-up and drop-off locations, as opposed to the public transportation vehicles. As a result, meeting the demand of the passengers by carefully distributing taxi vehicles around a crowded city like New York is an important issue. If there is an high demand in a certain part of the city, taxi service providers need to be able provide more vehicles to meet the demand. This would both increase the utilization of the available taxi resources and decrease the wait times of the passengers in return.\\ 

\noindent These being said, there are many different variables that play a role in people's decision to take a taxi ride instead of walking in the streets or using the subway. Some of these variables can be the price of the transportation vehicle, time of the day, day of the week, proximity of the stations or weather conditions. Therefore, this is a great problem to utilize Machine Learning techniques to figure out the complex relationships between these variables and the human nature. By modelling this complex relationship, taxi service providers can start making predictions for the future and provide a better service for their customers while making more profit. Thus, we want to solve the real world problem about strategically distributing taxicabs around the New York City by predicting number of pick-ups based on the \textit{location} and \textit{weather conditions}. By this way, taxi drivers can focus more on the locations with higher pick-up counts to decrease passenger waiting times while utilizing most of the taxi resource available and making more profit.

\subsection{Research Objective}
\noindent In this project, the New York City taxi data provided by New York City government was used in conjunction with the historical daily New York City weather forecast provided by Weather Underground company. As mentioned before, it is important for a city to cleverly distribute the taxi services and this projects intends to ease the taxi distribution. The project aims to create a Machine Learning model that takes information regarding day of the week, weather conditions, and the location for the pickup, while predicting the pickup frequency for the given location, as an output. As a result, both taxi service providers can locate the crowded spots in the city while+++ passengers can locate the nearest least crowded neighborhood in the city. We aim to maximize the taxicab resourced utilized while minimizing the passenger wait times.


\subsection{Scoping the Project}
\noindent Having plotted the pickup location heatmap, we decided to limit our exploration to Manhattan. Figure \ref{fig:heatmap}, plotted with a random subset of data, shows Manhattan to have the hottest pickup locations. Although not visible below, another popular pickup area is the airports. However, given our focus on weather's effects, we deemed it was appropriate to leave airports out, as taxi pickups are most likely independent of weather conditions.\\

\begingroup
    \centering
    %width=\columnwidth
    \includegraphics[width=0.45\columnwidth]{images/heatmap.png}
    \captionof{figure}{Pickup frequency by zones}
    \label{fig:heatmap}
    \medskip
\endgroup

\noindent We chose to pursue this idea after visualizing the timeseries pattern of precipitation against the pickup counts. Figures below show promising correlations between ridership and precipitation on days of extreme weather conditions.


\begingroup
    \centering
    \medskip
    %width=\columnwidth
    \subfloat{{\includegraphics[width=0.45\columnwidth]{report/images/2017_rain_count.png} }}%
    \qquad
    \subfloat{{\includegraphics[width=0.45\columnwidth]{report/images/2018_rain_count.png} }}%
    \caption{Precipitation vs. pickup counts in 2017 (left) and 2018 (right)}%
    \label{fig:curves}%
    \medskip
\endgroup


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Data Processing and Filtering %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Procurement, Processing, and Filtering}

\noindent Data was loaded into the workspace using the dask dataframe allowing for quicker access. As part of the \cc{read\_csv} function, the  \cc{tpep\_pickup\_datetime} and \cc{tpep\_dropoff\_datetime} columns were parsed as datetime formatted to allow for easier indexing and future data cleaning. Upon reviewing the dataframe, there were $1.846966e+07$ data points for 2017 and 2018 January. The challenge was then to identify methods for removing worthless datapoints and selecting a subset for training and testing. We decided against using the entire dataset because of limited computational resources. 

\subsection{Checking for Anomalies in TLC Data}

\noindent Once it was confirmed that there were no null values present in the 2017 and 2018 January dataset, we selected a 80\% fraction of the dataset to build our code with. We then sorted and reindexed the data to obtain a dataframe of purely January datapoints, removing stray datapoints from different months or years.\\

\noindent To aid in finding unreasonable data, we turned to the Adopted Rules of the Taxi and Limousine Comission (TLC). The maximum trip duration is 12 hours. Hence, we iterated through all columns to determine the trip duration by subtracting pickup time from dropoff time. Any duration greater than 12 hours (or in negative time) was deemed to be garbage and was removed from the dataset due to the regulation that was set by New York City government. In each iteration we also calculated the speed and removed all results with values less than 0 or greater than 90 miles per hours\\

\noindent For the remaining columns, such as \cc{total\_amount} and \cc{extra}, all rows with values less than zero were removed. 

\subsection{Shapefile Data}

\noindent Having little experience with shapefiles, we relied on functions from an existing machine learning project to manipualate the shapefile and produce maps with a pick up heatmap. 

\subsection{Weather Data}

\noindent Despite being relatively simple data, we ran into some trouble obtaining weather data for New York. As we had decided to focus on Manhattan, we found it fit to select a weather station there and select data between 2017 and 2018.\\

\noindent The data was obtained from the Weather Underground company website using their custom history viewing tool. Later, the data from the website was copied into a spreadsheet for preparing the data set. After cleaning the the copied data, an CSV version of the data set was created and imported. \\

\noindent Examining our dataset, we found null values for some days. Instead of removing them, we made the decision to linearly interpolate the data so as not to render corresponding values in our taxi dataframe unusable. Additionally, some of the values in the precipitation column were also "T", instead of numerical values. We deleted the rows with invalid precipitation data but it was not enough to use the weather dataset with the project. Pandas was reading the index column as an object rather than an float value. Objects can be anything and float values are needed to to work with Machine Learning algorithms. Finally, casting the precipitation column to string and later to float gave us the results we needed to run the algorithms.\\



\noindent Next, we combined the date, month, and year columns to produce a datetime column. In order to correspond the weather data with the TLC data, we used the \cc{pd.to\_datetime} method to obtain an indexable column. Using this column, we then merged  the relevant rain, average temperature, and average humidity data into the main dataframe.

%%%%%%%%%%%%%%%%%
%% Methodology %%
%%%%%%%%%%%%%%%%%
\section{Methodology}

\subsection{Data Visualization}

\noindent After meticulous cleaning and data vetting, we produced a dataframe combining taxi and weather data named \cc{nyc\_df}. We used this data to visualize the relation between ridership and the three weather features. The following figures allow for a rough visualization of any time series correlations. \\

\begingroup
    \centering
    \medskip
    %width=\columnwidth
    \subfloat{{\includegraphics[width=0.45\columnwidth]{report/images/2017_temp_count.png} }}%
    \qquad
    \subfloat{{\includegraphics[width=0.45\columnwidth]{report/images/2017_temp_count.png} }}%
    \caption{Average temperature vs. pickup counts in 2017 and 2018}%
    \label{fig:curves}%
    \medskip
\endgroup



\begingroup
    \centering
    \medskip
    %width=\columnwidth
    \subfloat{{\includegraphics[width=0.45\columnwidth]{report/images/2017_humidity_count.png} }}%
    \qquad
    \subfloat{{\includegraphics[width=0.45\columnwidth]{report/images/2017_temp_count.png} }}%
    \caption{Average humidity vs. pickup counts in 2017 and 2018}%
    \label{fig:curves}%
    \medskip
\endgroup


\subsection{Training Preparation}

\noindent After obtaining these timeseries visualizations we were adamant to move forward with the project, as the data seemed to suggest that extreme weather events affected ridership. For example, days with high humidity and precipitation tend to reduce ridership. This insight, combined with the inference that extreme weather reduces taxi mobility, implies that it becomes more imperative for.\\

\noindent We used \cc{nyc\_df} to produce \cc{df\_17} and \cc{df\_18}, both of which contain the day of the month, locationID, rainfall, average temperature, average humidity, and pickup count, for the years 2017 and 2018, respectively. Figure \ref{fig:train_test_df} shows how the dataframes look. \\

\begingroup
    \centering
    %width=\columnwidth
    \includegraphics[width=0.5\columnwidth]{report/images/train_test_df.png}
    \captionof{figure}{Snippets of df\_2017 and df\_2018}
    \label{fig:train_test_df}
    \medskip
\endgroup

\noindent Instead of the traditional \cc{train\_test\_split} method, we opted to make 2017 the training data and 2018 the testing data. This allows for us to test how well our model performs given how an entire month looks like. If this were to be used as an application or software, this implementation would make the most sense, as the user would simply input the aggregate data of the current year.

\subsection{Linear Regression}

\noindent Linear regression makes the assumption that there is a linear correlation between the independent and dependent variable. While this may seem easily dismissable, it is interesting to dissect this model. Looking at the data visualizations in the sections above, we noticed that on heavy weather event days, there was lower ridership. Hence, it may not be completely absurd to apply a linear fit to the model. However, because we also include day of the month and location ID as features, it becomes less reasonable to use a linear regression model.\\

\noindent For this implementation we simply used the \cc{squared\_loss} as the loss function and applied a grid search over a range of alpha values, optimizing for negative mean absolute error with 3-cross-fold validation. We then calculated the \cc{mean\_absolute\_errors} and \cc{mean\squared\_errors} for the training and test fits.

\subsection{Random Forest Regression}

\noindent Random forest regression operates on the idea of multiple decision trees. Each of these trees is built by randomly selecting data (with replacement) to create a bootstrapped dataset. Randomly selected features from the bootstrapped dataset are then used to decide what the root node for the decision tree is. This process is repeated multiple times to obtain a "forest". The data is then used to run down each of the trees and obtain the final answer.\\

\noindent For our experiment we searched through\cc{[10, 40, 80, 160, 320]} to find the optimal number of trees in the forest, seeking to minimize the negative mean absolute error and using 3 folds cross validation. We then trained a model with this and obtained the \cc{mean\_absolute\_errors} and \cc{mean\squared\_errors} for the training and test fits along with the accuracy. We also sought the out of bag error, which simply tests datapoints on trees that did not use those datapoints to build their bootstrap dataset for a score. Moreover, we also extracted the feature importances to view how important weather data was for our experiment when compared with day and location ID.

\subsection{XGBoost Regression}

\noindent XGBoost Regression is built upon the idea of decision trees, but it incorporates the errors of previous trees to build better ones in the future. Our XGBoost model takes the average of all pick up counts. The next tree built simply takes the errors into account by subtracting the calculated pick up average by the actual amount for each datapoint. However, the depth of each tree is specified. In each tree, each residual is added to the sum to obtain a guess. However, the XGBoost Regressor model avoids the high variance problem (overfitting problem) by assigning a learning rate to each of the trees so only small steps are taken in (hopefully) the right direction.\\

\noindent For our model, we searched through [4, 8, 10, 15, 20, 25, 32] as options for depth and [40, 80, 150, 600] as options for number of trees, minimizing for squared error and using 3 folds cross validation. We then obtained the the \cc{mean\_absolute\_errors} and \cc{mean\squared\_errors}, as well as the accuracy scores to test for performance. 

\subsection{KNeighbors Regression}

\noindent Eventually, we also used the K Nearest Neighbors Regressor to predict the pickup count. The idea of KNR significantly deviates from decision trees, because KNR makes its decisions based on how close another datapoint is numerically. At first thought, this may seem to work because the algorithm would likely correlate location ID and give counts based on how close it is to another location. However, looking at the location file, we see that the IDs are not assigned based geographically. In other words, location 1 is not necessarily geographically close to location 2. 

\subsection{Expectations}

\noindent From the analyses above, we did not expect promising results from the linear and KN regression models because of their assumptions. We did expect reasonable results from the decision-tree-based models because of their approach. 

%%%%%%%%%%%%%
%% Results %%
%%%%%%%%%%%%%
\section{Initial Results}

\noindent Below, MAPE refers to mean absolute percentage error and MSE refers to the mean squared error.

\subsection{Linear Regression}

\noindent GridSearchCV selected alpha = 130 and produced the following results\\


\begingroup
    \medskip
    \centering
    \def\arraystretch{1.5}
        \begin{tabular}{ccccc}
            \toprule
            TrainMAPE & TrainMSE & TestMAPE & TestMSE & R^2 \\
            \midrule
            70.58 & 591878 & 75.31 & 528067 & -0.014\\
            \bottomrule
        \end{tabular}
    \captionof{table}{Linear regression results}
    \label{table:}
    \medskip
\endgroup

\subsection{Random Forest Regression} 

\noindent GridSearchCV initially selected for 320 trees. The range of tree selection was then updated to   values = [320, 350, 400, 500, 600, 700, 800, 900], from which it selected 400 trees. Although, this value did not remain consisten after multiple runs.\\

\begingroup
    \medskip
    \centering
    \def\arraystretch{1.5}
        \begin{tabular}{ccccccc}
            \toprule
            TrainMAPE & TrainMSE & TestMAPE & TestMSE & Train Score & Test Score & OOB Score\\
            \midrule
            27.94 & 95001 & 77.89 & 585321 & 0.84 & -0.12 & -0.1929\\
            \bottomrule
        \end{tabular}
    \captionof{table}{Random Forest regression results}
    \label{table:}
    \medskip
\endgroup

\noindent We also plotted the feature importances for the random forest to see how the regression model weighted each feature. We learned that the model weighed LocationID as the most important feature, followed by day, humidity, temperature.\\

\begingroup
    \centering
    %width=\columnwidth
    \includegraphics[width=0.5\columnwidth]{report/images/feature_importance_rf.png}
    \captionof{figure}{Feature importance in Random Forest Model}
    \label{fig:train_test_df}
    \medskip
\endgroup

\subsection{XGBoost Regression}

\noindent XGBoost selected for 40 estimators with a tree depth of 1.\\


\begingroup
    \medskip
    \centering
    \def\arraystretch{1.5}
        \begin{tabular}{ccccc}
            \toprule
            TrainMAPE & TrainMSE & TestMAPE & TestMAPE & R^2 \\
            \midrule
            70.55 & 592156 & 75.23 & 527607 & -0.013\\
            \bottomrule
        \end{tabular}
    \captionof{table}{}
    \label{table:fifty_runs}
    \medskip
\endgroup


\noindent Looking back, we realized that our results were wholly unsatisfactory. Each of the models seemed to perform surprisingly poorly with high error and practically no accuracy. Hence, we returned to re-evaluate the issues we may have caused or any extra analyses we may have been missing.\\

\noindent After some discussion we decided that we had to make the following changes to our approach: 

\begin{itemize}
  \item We should train on 2017 and 2018, then test with 2019. Again, the model has too little experience to make any meaningful predictions. 
  \item Perhaps, we can use more than just January for training data. The models have too little experience with the days of the months.
  \item We can use PCA to find the best weather features, instead of picking certain ones. 
  \item Our data is organized by day and cross validation is not shuffling the data. Hence, there may be a biased and misleading set of parameters being used for the model, leading to suboptimal performance
  
\end{itemize}

\section{Revised Approach}

\noindent After evaluating our options and our project timeframe, we opted to make three changes to our model. We included 2019 data and used a combination of 2017 and 2018 for training, while using 2019 for testing. We added a feature called day of the week, under the assumption that taxi data may be correlated with what day of the week is because of a cyclic pattern. Finally, we updated our 3 cross fold validation to shuffle the data. We also included results from the KNR approach to see if it fared better than the other models.\\

\subsection{Linear Regression}
\noindent GridSearchCV selected alpha = 1e-14 and produced the following results\\


\begingroup
    \medskip
    \centering
    \def\arraystretch{1.5}
        \begin{tabular}{ccccc}
            \toprule
            TrainMAPE & TrainMSE & TestMAPE & TestMSE & R^2 \\
            \midrule
            71.80 & 1410132 & 81.74 & 1046655 & -0.075\\
            \bottomrule
        \end{tabular}
    \captionof{table}{Linear regression results after revisions}
    \label{table:}
    \medskip
\endgroup


\subsection{Random Forest Regression} 

\noindent GridSearchCV selected for 400 trees.

\begingroup
    \medskip
    \centering
    \def\arraystretch{1.5}
        \begin{tabular}{cccccCC}
            \toprule
            TrainMAPE & TrainMSE & TestMAPE & TestMSE &  Train Score & Test Score & OOB Score\\
            \midrule
            27.92 & 217035 & 82.58 & 1112606 & 0.84 & -0.14 & -0.12\\
            \bottomrule
        \end{tabular}
    \captionof{table}{Random Forest regression results after revisions}
    \label{table:}
    \medskip
\endgroup



\begingroup
    \centering
    %width=\columnwidth
    \includegraphics[width=0.5\columnwidth]{report/images/rev_feature_importance_rf.png}
    \captionof{figure}{Feature importance in Random Forest Model after revisions}
    \label{fig:train_test_df}
    \medskip
\endgroup

\subsection{XGBoost Regression}

\noindent XGBoost selected for 40 estimators with a tree depth of 3.\\


\begingroup
    \medskip
    \centering
    \def\arraystretch{1.5}
        \begin{tabular}{ccccc}
            \toprule
            TrainMAPE & TrainMSE & TestMAPE & TestMAPE & R^2 \\
            \midrule
            68.98 & 1319462 & 79.92 & 1008289 & -0.035\\
            \bottomrule
        \end{tabular}
    \captionof{table}{}
    \label{table:fifty_runs}
    \medskip
\endgroup


\subsection{KNeighbors Regression}

\noindent GridSearch selected for 40 neighbors\\


\begingroup
    \medskip
    \centering
    \def\arraystretch{1.5}
        \begin{tabular}{cccccc}
            \toprule
            TrainMAPE & TrainMSE & TestMAPE & TestMAPE & Train Accuracy & Test Accuracy \\
            \midrule
            70.06 & 1347247 & 83.80 & 1103577 & 0.059 & -0.13\\
            \bottomrule
        \end{tabular}
    \captionof{table}{}
    \label{table:fifty_runs}
    \medskip
\endgroup


\subsection{Comparing Mean Absolute Percentage Error (MAPE)}

\begingroup
    \centering
    \medskip
    %width=\columnwidth
    \subfloat{{\includegraphics[width=0.45\columnwidth]{report/images/mape.png} }}%
    \qquad
    \subfloat{{\includegraphics[width=0.45\columnwidth]{report/images/rev-mape.png} }}%
    \caption{Test MAPE for all used models}%
    \label{fig:curves}%
    \medskip
\endgroup

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Discussion and Conclusion %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusion}

\noindent To our surprise, the revisions we made resulted in increasing the Test MAPE and Train MAPE values. From this we can draw that perhaps in the initial attempt there was overfitting, resulting in a much lower Train MAPE. The increase in Test MAPE may also be a sign implying that there was overfitting in the original attempt, which falsely led to a lower MAPE. We decided to focus on the MAPE metric over accuracy because we were carrying out regression rather than classification. In the case of the latter, data points are placed into discrete bins, where it makes sense to use accuracy. In our case, the model is predicting an output from a range of values, where it is highly likely to never output the exact output. Hence, we use the percentage error as a metric to gauge how close it is to the ideal solution.\\

\noindent Overall, we believe our lackluster results were a combination of too little data and attempting to find a correlation where one does not exist. Other projects have shown that there is some but insignificant correlation between ridership and weather. Despite this, we sought to find a method based on weather data in curiosity to see if it would be possible. Although our results do not verify our claim that it is not possible, they do imply that it is not necessarily a simple task. 



\printbibliography


%%%%%%%%%%%%%%% figure example %%%%%%%%%%%%%%%%%%%%%%%%

% \begingroup
%     \centering
%     %width=\columnwidth
%     \includegraphics[width=\columnwidth]{images/scale-free.png}
%     \captionof{figure}{Scale-free network with n=500, e=1500. Size and color of each is proportional to that node's degree. Bigger and darker node have more connections. Smaller and lighter nodes have fewer connections.}
%     \label{fig:scalefree_network}
% \endgroup


%%%%%%%%%%%%%%% equation example %%%%%%%%%%%%%%%%%%%%%%%%

% \begin{equation}
%     \begin{split}
%         X &\texttt{\char`\~} Poisson(\lambda) \\
%         \\
%         P(X=k) &= \frac{\lambda^k e^{-\lambda}}{k!} \\
%         \\
%         \EX(k) &= e^{-\lambda}\sum_{k=1}^{\infty} \frac{\lambda^n}{(n-1)!}\\
%         \\
%         &= e^{-\lambda}\lambda\sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}\\
%         \\
%         &= e^{-\lambda}\lambda\sum_{n=0}^{\infty} \frac{\lambda^{n}}{(n)!}\\
%         \\
%         &= e^{-\lambda}\lambda e^{\lambda} = \lambda\\
%     \end{split}
%     \label{eq:mutual}
% \end{equation}

%%%%%%%%%%%%%%% table example %%%%%%%%%%%%%%%%%%%%%%%%

% \begingroup
%     \medskip
%     \centering
%     \def\arraystretch{1.5}
%         \begin{tabular}{lcc}
%             \toprule
%             & Random Network & Scale-Free Network \\
%             \midrule
%             Avg. of Avg. Degree    & 3.9988    &  4.0035   \\
%             Var. of Avg. Degree    & 7.6800$\times 10^{-7}$ & 3.1156$\times 10^{-6}$  \\
%             Avg. of Avg. Distance  & 5.5694 & 5.0935    \\
%             Var. of Avg. Distance  & 1.5507 & 0.5377   \\
%             \bottomrule
%         \end{tabular}
%     \captionof{table}{Statistics of fifty runs where \\ n = 5,000, e = 10,000}
%     \label{table:fifty_runs}
%     \medskip
% \endgroup

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}