{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile\n",
    "from shapely.geometry import Polygon\n",
    "from descartes.patch import PolygonPatch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import socket\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import bayesian_optimization\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import sklearn.model_selection as cv\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion for cross-validation over a grid of parameters\n",
    "\n",
    "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None, verbose=0):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func, verbose=verbose)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds, verbose=verbose)\n",
    "    gs.fit(X, y)\n",
    "    print(\"BEST\", gs.best_params_, gs.best_score_, gs.cv_results_, gs.scorer_)\n",
    "    print(\"Best score: \", gs.best_score_)\n",
    "    best = gs.best_estimator_\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Trip Record Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmonth = 1\\nfor year in range(2017, 2020):\\n    urllib.request.urlretrieve(\"https://s3.amazonaws.com/nyc-tlc/trip+data/\"+                                \"yellow_tripdata_{0:4d}-{1:0=2d}.csv\".format(year, month), \\n                               \"nyc.{0:4d}-{1:0=2d}.csv\".format(year, month))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For downloading TLC data for multiple months in the same year\n",
    "'''\n",
    "for month in range(1,2):\n",
    "    urllib.request.urlretrieve(\"https://s3.amazonaws.com/nyc-tlc/trip+data/\"+ \\\n",
    "                               \"yellow_tripdata_2018-{0:0=2d}.csv\".format(month), \n",
    "                               \"nyc.2018-{0:0=2d}.csv\".format(month))\n",
    "'''\n",
    "\n",
    "# For downloading TLC data for a single month in multiple years\n",
    "'''\n",
    "month = 1\n",
    "for year in range(2017, 2020):\n",
    "    urllib.request.urlretrieve(\"https://s3.amazonaws.com/nyc-tlc/trip+data/\"+ \\\n",
    "                               \"yellow_tripdata_{0:4d}-{1:0=2d}.csv\".format(year, month), \n",
    "                               \"nyc.{0:4d}-{1:0=2d}.csv\".format(year, month))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "nyc_df = pd.read_csv(\"datasets/nyc.2017-01.csv\", parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "nyc_df = nyc_df.set_index('tpep_pickup_datetime')\n",
    "nyc_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the rows that don't belong to the choosen time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df = nyc_df.loc['2018-01']\n",
    "nyc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the data based on the date information and reindex it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df.sort_values(by=['tpep_pickup_datetime'])\n",
    "nyc_df.reset_index(inplace=True)\n",
    "nyc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for anomalies in the dataset and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = []\n",
    "\n",
    "for index, row in nyc_df.iterrows():\n",
    "    duration = nyc_df.tpep_dropoff_datetime[index] - nyc_df.tpep_pickup_datetime[index]\n",
    "\n",
    "    if(duration > datetime.timedelta(hours=12)):\n",
    "        rows_to_drop.append(index)\n",
    "        \n",
    "    elif(duration.seconds != 0):\n",
    "        # Miles per hour, average speed\n",
    "        speed = (nyc_df.trip_distance[index]/duration.seconds)*60\n",
    "        \n",
    "        # Check if a taxi is going faster than they can in real life\n",
    "        if((speed > 90.0) or (speed < 1.0)):\n",
    "            rows_to_drop.append(index)\n",
    "            \n",
    "nyc_df.drop(nyc_df.index[rows_to_drop])\n",
    "\n",
    "nyc_df = nyc_df[nyc_df['total_amount'] >= 0]\n",
    "nyc_df = nyc_df[nyc_df['extra'] >= 0]\n",
    "nyc_df = nyc_df[nyc_df['mta_tax'] >= 0]\n",
    "nyc_df = nyc_df[nyc_df['fare_amount'] >= 0]\n",
    "nyc_df = nyc_df[nyc_df['tolls_amount'] >= 0]\n",
    "nyc_df = nyc_df[nyc_df['improvement_surcharge'] >= 0]\n",
    "\n",
    "nyc_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Download the location Data\n",
    "urllib.request.urlretrieve(\"https://s3.amazonaws.com/nyc-tlc/misc/taxi_zones.zip\", \"taxi_zones.zip\")\n",
    "with zipfile.ZipFile(\"taxi_zones.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./datasets/taxi_zones/shape\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lon(sf):\n",
    "    content = []\n",
    "    for sr in sf.shapeRecords():\n",
    "        shape = sr.shape\n",
    "        rec = sr.record\n",
    "        loc_id = rec[shp_dic['LocationID']]\n",
    "        \n",
    "        x = (shape.bbox[0]+shape.bbox[2])/2\n",
    "        y = (shape.bbox[1]+shape.bbox[3])/2\n",
    "        \n",
    "        content.append((loc_id, x, y))\n",
    "    return pd.DataFrame(content, columns=[\"LocationID\", \"longitude\", \"latitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert shape file to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = shapefile.Reader(\"datasets/taxi_zones/shape/taxi_zones.shp\")\n",
    "fields_name = [field[0] for field in sf.fields[1:]]\n",
    "shp_dic = dict(zip(fields_name, list(range(len(fields_name)))))\n",
    "attributes = sf.records()\n",
    "shp_attr = [dict(zip(fields_name, attr)) for attr in attributes]\n",
    "\n",
    "loc_df = pd.DataFrame(shp_attr).join(get_lat_lon(sf).set_index(\"LocationID\"), on=\"LocationID\")\n",
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove location id's that are outside of the Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df = loc_df[loc_df.borough == \"Manhattan\"]\n",
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert location ids that belong to Manhattan into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_loc_id_list = loc_df[\"LocationID\"].tolist()\n",
    "print(manhattan_loc_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersect NYC Taxi and NYC Taxi Zone Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove taxi rides that didn't originate from Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df = nyc_df[nyc_df['PULocationID'].isin(manhattan_loc_id_list)]\n",
    "\n",
    "nyc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv(\"datasets/weather_data.csv\")\n",
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the types of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert events column into multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['event_rain'] = 0\n",
    "weather_df['event_fog'] = 0\n",
    "weather_df['event_snow'] = 0\n",
    "\n",
    "for index, row in weather_df.iterrows():\n",
    "    if(isinstance(weather_df.events[index], str)):\n",
    "        if(\"Rain\" in weather_df.events[index]):\n",
    "            weather_df.event_rain[index] = 1\n",
    "\n",
    "        if(\"Fog\" in weather_df.events[index]):\n",
    "            weather_df.event_fog[index] = 1\n",
    "\n",
    "        if(\"Snow\" in weather_df.events[index]):\n",
    "            weather_df.event_snow[index] = 1\n",
    "\n",
    "weather_df = weather_df.drop(\"events\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add primary key to be used in the NYC taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.insert(0, \"primary_key\", \"\") \n",
    "\n",
    "for index, row in weather_df.iterrows():\n",
    "    key = str(row['year'])\n",
    "    \n",
    "    if (row['month'] == 0) or (row['month'] == 1) or (row['month'] == 2) or (row['month'] == 3) or (row['month'] == 4) or (row['month'] == 5) or (row['month'] == 6) or (row['month'] == 7) or (row['month'] == 8) or (row['month'] == 9):\n",
    "        key = key + \"-0\" + str(row['month'])\n",
    "    else:\n",
    "        key = key + \"-\" + str(row['month'])\n",
    "        \n",
    "    if (row['day'] == 0) or (row['day'] == 1) or (row['day'] == 2) or (row['day'] == 3) or (row['day'] == 4) or (row['day'] == 5) or (row['day'] == 6) or (row['day'] == 7) or (row['day'] == 8) or (row['day'] == 9):\n",
    "        key = key + \"-0\" + str(row['day'])\n",
    "    else:\n",
    "        key = key + \"-\" + str(row['day'])\n",
    "        \n",
    "    weather_df.primary_key[index] = key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the data based on the date information and reindex it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_df['primary_key'] = pd.to_datetime(weather_df['primary_key'])\n",
    "weather_df = weather_df.set_index('primary_key')\n",
    "\n",
    "#weather_df.reset_index(inplace=True)\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill rows with missing values using interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_df.interpolate(method=\"linear\")\n",
    "\n",
    "#weather_df = weather_df.dropna()\n",
    "weather_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Weather Data to NYC Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df['rain'] = 0\n",
    "nyc_df['temperature_avg'] = 0\n",
    "nyc_df['humidity_avg'] = 0\n",
    "nyc_df.insert(0, \"day\", 0) \n",
    "\n",
    "for index, row in nyc_df.iterrows():\n",
    "    string_key = nyc_df.tpep_pickup_datetime[index].strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    row_array = weather_df.loc[string_key]\n",
    "    \n",
    "    nyc_df.rain[index] = row_array[\"precipitation\"]\n",
    "    nyc_df.temperature_avg[index] = row_array[\"temp_avg\"]\n",
    "    nyc_df.humidity_avg[index] = row_array[\"humidity_avg\"]\n",
    "    nyc_df.day[index] = int(nyc_df.tpep_pickup_datetime[index].strftime(\"%d\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nyc_df.tpep_pickup_datetime.count()\n",
    "#nyc_df.tpep_pickup_datetime[0].strftime(\"%Y-%m-%d\")\n",
    "#weather_df.primary_key[0].strftime(\"%Y-%m-%d\")\n",
    "#weather_df.dtypes\n",
    "#weather_df.loc[\"2016-01-10\"]\n",
    "\n",
    "nyc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce the target data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the frequency values of locations based on days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = nyc_df[['day', 'PULocationID']].copy()\n",
    "temp_df = temp_df.drop_duplicates(subset=['day', 'PULocationID'], keep='first')\n",
    "temp_df['freq'] = 0\n",
    "\n",
    "# Iterate over the unique location and day information\n",
    "for index, row in temp_df.iterrows():\n",
    "    day = temp_df.day[index]\n",
    "    location_id = temp_df.PULocationID[index]\n",
    "    \n",
    "    # Get rows from NYC taxi data with matching days\n",
    "    day_temp_df = nyc_df.loc[nyc_df['day'] == day]\n",
    "    \n",
    "    # Count the number of rows with matching PULocationID within the matching days\n",
    "    count = len(day_temp_df.loc[day_temp_df['PULocationID'] == location_id])\n",
    "\n",
    "    temp_df.freq[index] = count\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map frequency data to NYC taxi data to match the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.DataFrame()\n",
    "target_df['freq'] = 0\n",
    "\n",
    "# Iterate over the NYC taxi data\n",
    "for index, row in nyc_df.iterrows():\n",
    "    day = nyc_df.day[index]\n",
    "    location_id = nyc_df.PULocationID[index]\n",
    "    \n",
    "    day_temp_df = temp_df.loc[temp_df['day'] == day]\n",
    "    location_temp = day_temp_df.loc[day_temp_df['PULocationID'] == location_id]\n",
    "        \n",
    "    target_df.loc[index] = location_temp.freq.values\n",
    "\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unused features before the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df = nyc_df.drop(columns=['tpep_pickup_datetime',\n",
    "                              'tpep_dropoff_datetime',\n",
    "                              'DOLocationID',\n",
    "                              'VendorID',\n",
    "                              'RatecodeID',\n",
    "                              'store_and_fwd_flag',\n",
    "                              'payment_type',\n",
    "                              'passenger_count',\n",
    "                              'fare_amount',\n",
    "                              'total_amount',\n",
    "                              'trip_distance',\n",
    "                              'extra',\n",
    "                              'mta_tax',\n",
    "                              'tip_amount',\n",
    "                              'tolls_amount',\n",
    "                              'improvement_surcharge'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target=nycmodel[['count']]\n",
    "#data=nycmodel[[col for col in nycmodel.columns if col not in ['count']]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(nyc_df, target_df, test_size=2.0/10, random_state=5)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "y_predictedValue = reg.predict(x_train)  \n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_predictedValue))\n",
    "r2 = reg.score(x_train, y_train)\n",
    "\n",
    "print(\"The model performance for training set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R^2 score is {}'.format(r2))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Regression estimator\n",
    "estimator = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n",
    "\n",
    "# Define a grid of parameters over which to optimize the random forest\n",
    "# We will figure out which number of trees is optimal\n",
    "parameters = {\"n_estimators\": [50],\n",
    "              \"max_features\": [\"auto\"], # [\"auto\",\"sqrt\",\"log2\"]\n",
    "              \"max_depth\": [50]}\n",
    "best = cv_optimize(estimator, parameters, x_train, y_train, n_folds=5, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best Random Forest and calculate R^2 values for training and test sets\n",
    "reg=best.fit(x_train, y_train)\n",
    "training_accuracy = reg.score(x_train, y_train)\n",
    "test_accuracy = reg.score(x_test, y_test)\n",
    "print(\"############# based on standard predict ################\")\n",
    "print(\"R^2 on training data: %0.4f\" % (training_accuracy))\n",
    "print(\"R^2 on test data:     %0.4f\" % (test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XgBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "x_model = xgb.XGBRegressor()\n",
    "param_dist = {\"max_depth\": [3, 4,5],\n",
    "              'n_estimators': [randint(400,600)],\n",
    "              \"min_child_weight\": [3, 4,5,6],\n",
    "              \"gamma\":[0,0.1,0.2],\n",
    "              \"colsample_bytree\":[0.7,0.8,0.9],\n",
    "              \"nthread\":[3,4,5]\n",
    "              }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(x_model, param_distributions=param_dist,n_iter=n_iter_search)\n",
    "random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model = xgb.XGBRegressor(\n",
    "    n_estimators=513,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    gamma=0,\n",
    "    colsample_bytree=0.9,nthread=5)\n",
    "\n",
    "x_model.fit(x_train, y_train)\n",
    "y_pred = x_model.predict(x_train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r2 = reg.score(x_train, y_train)\n",
    "\n",
    "print(\"The model performance for training set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R^2 score is {}'.format(r2))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
